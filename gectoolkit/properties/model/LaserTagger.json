{
  "embedding_size": 128,
  "pretrained_model_path": "./gectoolkit/properties/model/LaserTagger",
  "train_batch_size": 32,
  "test_batch_size": 32,
  "approx": "none",
  "dropout": 0.1,
  "embed_dim": 768,
  "ff_embed_dim":3072,
  "log_dir": "log",

  "enable_swap_tag": false,
  "output_arbitrary_targets_for_infeasible_examples": false,
  "attention_probs_dropout_prob": 0.1,
  "max_seq_length": 128,
  "do_lower_case": true,
  "use_arbitrary_target_ids_for_infeasible_examples": true,
  "label_map_file": "gectoolkit/model/LaserTagger/tmp_output/label_map.txt",

  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 16534,
  "use_t2t_decoder": true,
  "decoder_num_hidden_layers": 1,
  "decoder_hidden_size": 768,
  "decoder_num_attention_heads": 4,
  "decoder_filter_size": 3072,
  "use_full_attention": false,

  "directionality": "bidi",
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "layer_postprocess_dropout":0.1,
  "attention_dropout":0.1,
  "relu_dropout":0.1,
  "allow_ffn_pad": true,

  "layers":12,
  "gamma": 0.5,
  "num_heads":12,
  "tpu": false
}